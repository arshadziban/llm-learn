{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aaa366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d8253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 5\n",
      "LlamaIndex Basics: An Introduction for\n",
      "Retrieval-Augmented Generation\n",
      "1. Introduction to LlamaIndex\n",
      "LlamaIndexis a framework designed to connect Large Language Models (LLMs) with\n",
      "external data sources such as PDF documents, text files, databases, and web content.\n",
      "In essence, LlamaIndex enables an LLM to read, search, and generate answers based\n",
      "on user-provided documents. It is primarily used for buildingRetrieval-Augmented\n",
      "Generation (RAG)systems.\n",
      "2. Motivation for Using LlamaIndex\n",
      "By default, L\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"02_1_LlamaIndex.pdf\"]).load_data()\n",
    "\n",
    "print(\"Documents loaded:\", len(documents))\n",
    "print(documents[0].text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94dd699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-24 13:19:13,030 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29776655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-24 13:19:19,177 - WARNING - Supplied context_window 3900 is greater than the model's max input size 2048. Disable this warning by setting a lower context_window.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    tokenizer_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19f7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff92e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "817fe350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      "\n",
      "LlamaIndex is a framework designed to connect Large Language Models (LLMs) with external data sources such as PDF documents, text files, databases, and web content. It enables an LLM to read, search, and generate answers based on user-provided documents. LlamaIndex addresses challenges such as data indexing, retrieval, and generation, and is suitable for document-centric question answering, complex multi-step workflows, and beginner-friendly workflows. LlamaIndex is suitable for projects that require document-centric question answering, research and study assistants, and knowledge-base chatbots. LlamaIndex is not a Large Language Model and does not generate text independently. It serves as a bridge between external data and LLMs. LlamaIndex is suitable for projects that require document-centric question answering, research and study assistants, and knowledge-base chatbots.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LlamaIndex?\"\n",
    "\n",
    "response = query_engine.query(question)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
