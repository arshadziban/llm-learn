{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef0510d",
   "metadata": {},
   "source": [
    "# LangChain-Based Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a complete implementation of a Retrieval-Augmented Generation (RAG) system using LangChain. The system retrieves relevant information from PDF documents and generates contextual answers using a language model.\n",
    "\n",
    "## Architecture\n",
    "1. **Document Loading**: Load and process PDF documents\n",
    "2. **Text Chunking**: Split documents into manageable chunks\n",
    "3. **Embedding Generation**: Convert text chunks into vector representations\n",
    "4. **Vector Storage**: Store embeddings in a FAISS vector database\n",
    "5. **Retrieval**: Query the database for relevant documents\n",
    "6. **Generation**: Use an LLM to generate answers based on retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52d2ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35f051",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Document Processing Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65f44fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"01_lang_chain.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38eea1b",
   "metadata": {},
   "source": [
    "## Step 2: Load and Parse PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cf33c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 5\n",
      "LangChain Basics: A Framework for\n",
      "Retrieval-Augmented Generation\n",
      "1. Introduction to LangChain\n",
      "LangChainis an open-source framework designed to support the development of ap-\n",
      "plications powered by Large Language Models (LLMs) in a structured, modular, and\n",
      "scalable manner.\n",
      "Its primary objective is to facilitate the integration of LLMs with external components\n",
      "such as documents, vector databases, application programming interfaces (APIs), tools,\n",
      "and memory systems. LangChain is widely used in the i\n"
     ]
    }
   ],
   "source": [
    "print(\"Total pages:\", len(documents))\n",
    "print(documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35680a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks generated: 14\n",
      "First chunk sample (446 characters):\n",
      "LangChain Basics: A Framework for\n",
      "Retrieval-Augmented Generation\n",
      "1. Introduction to LangChain\n",
      "LangChainis an open-source framework designed to support the development of ap-\n",
      "plications powered by Large Language Models (LLMs) in a structured, modular, and\n",
      "scalable manner.\n",
      "Its primary objective is to \n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Maximum tokens per chunk\n",
    "    chunk_overlap=50  # Overlap between adjacent chunks to maintain context\n",
    ")\n",
    "\n",
    "# Split documents into semantically coherent chunks\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total chunks generated: {len(chunks)}\")\n",
    "print(f\"First chunk sample ({len(chunks[0].page_content)} characters):\")\n",
    "print(chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51a0e8",
   "metadata": {},
   "source": [
    "## Step 3: Implement Recursive Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77205a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"  # Efficient model for semantic similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc299d3f",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "090302f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    chunks,\n",
    "    embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c40a26",
   "metadata": {},
   "source": [
    "## Step 5: Build Vector Database (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36fe9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Langchain?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b5a6e",
   "metadata": {},
   "source": [
    "## Step 6: Define Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92bbdf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = vectorstore.similarity_search_with_score(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e01c1",
   "metadata": {},
   "source": [
    "## Step 7: Retrieve Relevant Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea4b9487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize text generation pipeline using GPT-2\n",
    "# Note: GPT-2 is used for demonstration; consider larger models for production\n",
    "text_gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    max_new_tokens=150  # Only generate 150 new tokens (not counting input)\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_gen_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4396713",
   "metadata": {},
   "source": [
    "## Step 8: Initialize Language Model for Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c42de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional technical expert providing comprehensive, well-structured responses.\n",
      "\n",
      "Analyze the following context and provide a detailed, formal answer to the question. \n",
      "\n",
      "Requirements:\n",
      "- Maintain a professional and academic tone\n",
      "- Use clear, precise terminology\n",
      "- Structure your response with logical flow\n",
      "- Cite relevant information from the context\n",
      "- If the answer cannot be derived from the provided context, state \"The requested information is not available in the provided documentation.\"\n",
      "\n",
      "Context:\n",
      "8. Summary\n",
      "LangChain is a modular framework that facilitates the development of LLM-powered\n",
      "applications, particularly Retrieval-Augmented Generation systems. By separating data\n",
      "ingestion, retrieval, and generation into reusable components, LangChain enables devel-\n",
      "opers to build scalable, maintainable, and robust language-based applications.\n",
      "Exam-Ready One-Line Summary\n",
      "LangChain is a modular framework that integrates Large Language Models\n",
      "\n",
      "LangChain Basics: A Framework for\n",
      "Retrieval-Augmented Generation\n",
      "1. Introduction to LangChain\n",
      "LangChainis an open-source framework designed to support the development of ap-\n",
      "plications powered by Large Language Models (LLMs) in a structured, modular, and\n",
      "scalable manner.\n",
      "Its primary objective is to facilitate the integration of LLMs with external components\n",
      "such as documents, vector databases, application programming interfaces (APIs), tools,\n",
      "\n",
      "and memory systems. LangChain is widely used in the implementation of Retrieval-\n",
      "Augmented Generation (RAG) architectures.\n",
      "2. Motivation for Using LangChain\n",
      "Using a standalone LLM is often insufficient for real-world applications due to several\n",
      "practical challenges:\n",
      "•LLMs do not have access to private or proprietary data by default\n",
      "•Prompt logic becomes complex and difficult to manage at scale\n",
      "•Coordinating retrieval, memory, tools, and generation requires additional infras-\n",
      "tructure\n",
      "\n",
      "Question: What is Langchain?\n",
      "\n",
      "Professional Answer: Langchain is a modular, multi-language implementation of Retrieval-Augmented\n",
      "\n",
      "Generation. It is based on a single-language implementation of LLMs, which is based on the\n",
      "\n",
      "language models of the many open source, distributed, and open source\n",
      "\n",
      "LLMs.\n",
      "\n",
      "Question: How can I use LangChain for my application?\n",
      "\n",
      "LangChain is based on the following LLMs:\n",
      "\n",
      "- C++17 x86 and x86_64\n",
      "\n",
      "- C#11 and C++17\n",
      "\n",
      "- C#11 and C++17\n",
      "\n",
      "- C++17\n",
      "\n",
      "- C#11\n",
      "\n",
      "- C++17\n",
      "\n",
      "- MSVC-C++\n",
      "\n",
      "- MS\n"
     ]
    }
   ],
   "source": [
    "# Define formal prompt template for context-aware answer generation\n",
    "template = \"\"\"You are a professional technical expert providing comprehensive, well-structured responses.\n",
    "\n",
    "Analyze the following context and provide a detailed, formal answer to the question. \n",
    "\n",
    "Requirements:\n",
    "- Maintain a professional and academic tone\n",
    "- Use clear, precise terminology\n",
    "- Structure your response with logical flow\n",
    "- Cite relevant information from the context\n",
    "- If the answer cannot be derived from the provided context, state \"The requested information is not available in the provided documentation.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Professional Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Aggregate retrieved documents to form the context window\n",
    "context = \"\\n\\n\".join([doc.page_content for doc, _ in retrieved_docs[:3]])\n",
    "\n",
    "# Generate answer using retrieval-augmented generation\n",
    "formatted_prompt = prompt.format(context=context, question=query)\n",
    "llm_answer = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Format and display results\n",
    "# print(f\"Query: {query}\")\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"Professional Answer (Retrieval-Augmented Generation):\")\n",
    "# print(\"=\"*70)\n",
    "print(llm_answer.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
