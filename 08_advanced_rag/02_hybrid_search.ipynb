{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c440fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eef3e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_files=[\"../07_rag_frameworks/02_1_LlamaIndex.pdf\"]).load_data()\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f1e739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-24 14:00:25,473 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2436d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"gpt2\",\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    max_new_tokens=100,\n",
    "    generate_kwargs={\"do_sample\": False, \"top_p\": 0.9}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e43da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "735756ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x198503eca90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84c2c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06858985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Retrieved Documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "LlamaIndex Basics: An Introduction for\n",
      "Retrieval-Augmented Generation\n",
      "1. Introduction to LlamaIndex\n",
      "LlamaIndexis a framework designed to connect Large Language Models (LLMs) with\n",
      "external data sources such as PDF documents, text files, databases, and web content.\n",
      "In essence, LlamaIndex enables an LL\n",
      "\n",
      "--- Document 2 ---\n",
      "6. Comparison with LangChain\n",
      "Aspect LlamaIndex LangChain\n",
      "Primary focus Data indexing and retrieval Workflow orchestration\n",
      "Ease of use High (beginner-friendly) Moderate\n",
      "RAG suitability Very strong Strong\n",
      "Abstraction level High Medium\n",
      "Pipeline flexibility Limited Highly flexible\n",
      "As a general guideline\n",
      "\n",
      "--- Document 3 ---\n",
      "10. Recommended Learning Path\n",
      "A suggested progression for learning LlamaIndex is:\n",
      "1. Understanding document ingestion and nodes\n",
      "2. Learning index construction\n",
      "3. Implementing query mechanisms\n",
      "4. Building PDF-based question answering systems\n",
      "5. Integrating a Large Language Model\n",
      "6. Adding citation an\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"what is LlamaIndex?\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "nodes = retriever.retrieve(question)\n",
    "\n",
    "print(\"Top 3 Retrieved Documents:\\n\")\n",
    "for i, node in enumerate(nodes, 1):\n",
    "    print(f\"--- Document {i} ---\")\n",
    "    print(node.get_content()[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04658bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Retrieved Document:\n",
      "\n",
      "Score: 0.6222384434677171\n",
      "\n",
      "Content:\n",
      "LlamaIndex Basics: An Introduction for\n",
      "Retrieval-Augmented Generation\n",
      "1. Introduction to LlamaIndex\n",
      "LlamaIndexis a framework designed to connect Large Language Models (LLMs) with\n",
      "external data sources such as PDF documents, text files, databases, and web content.\n",
      "In essence, LlamaIndex enables an LLM to read, search, and generate answers based\n",
      "on user-provided documents. It is primarily used for buildingRetrieval-Augmented\n",
      "Generation (RAG)systems.\n",
      "2. Motivation for Using LlamaIndex\n",
      "By default, L\n",
      "\n",
      "Metadata: {'page_label': '1', 'file_name': '02_1_LlamaIndex.pdf', 'file_path': '..\\\\07_rag_frameworks\\\\02_1_LlamaIndex.pdf', 'file_type': 'application/pdf', 'file_size': 75694, 'creation_date': '2025-12-24', 'last_modified_date': '2025-12-24'}\n"
     ]
    }
   ],
   "source": [
    "# Get the best retrieved document\n",
    "best_node = nodes[0]\n",
    "print(\" Best Retrieved Document:\\n\")\n",
    "print(f\"Score: {best_node.score}\")\n",
    "print(f\"\\nContent:\\n{best_node.get_content()[:500]}\")\n",
    "print(f\"\\nMetadata: {best_node.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cc15e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYBRID SEARCH COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Vector Search Results:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Score: 0.6222\n",
      "   Content: LlamaIndex Basics: An Introduction for\n",
      "Retrieval-Augmented Generation\n",
      "1. Introduction to LlamaIndex\n",
      "LlamaIndexis a framework designed to connect Large Language Models (LLMs) with\n",
      "external data sources such as PDF documents, text files, databases, and...\n",
      "\n",
      "2. Score: 0.5284\n",
      "   Content: 6. Comparison with LangChain\n",
      "Aspect LlamaIndex LangChain\n",
      "Primary focus Data indexing and retrieval Workflow orchestration\n",
      "Ease of use High (beginner-friendly) Moderate\n",
      "RAG suitability Very strong Strong\n",
      "Abstraction level High Medium\n",
      "Pipeline flexibil...\n",
      "\n",
      "3. Score: 0.5014\n",
      "   Content: 10. Recommended Learning Path\n",
      "A suggested progression for learning LlamaIndex is:\n",
      "1. Understanding document ingestion and nodes\n",
      "2. Learning index construction\n",
      "3. Implementing query mechanisms\n",
      "4. Building PDF-based question answering systems\n",
      "5. Integr...\n",
      "\n",
      "4. Score: 0.4812\n",
      "   Content: Instead of providing entire documents to an LLM, LlamaIndex:\n",
      "•Splits documents into smaller units\n",
      "•Stores them in an index\n",
      "•Retrieves only the most relevant parts for each query\n",
      "4. Main Components of LlamaIndex\n",
      "4.1 Document Loader\n",
      "The Document Loader...\n",
      "\n",
      "5. Score: 0.4444\n",
      "   Content: •Keyword Index\n",
      "•List Index\n",
      "The vector index is the most commonly used in RAG systems.\n",
      "4.4 Embeddings\n",
      "Text content is converted into dense numerical vectors known asembeddings.\n",
      "Embeddings enable:\n",
      "•Semantic similarity comparison\n",
      "•Meaning-based retrieva...\n",
      "\n",
      "\n",
      "Keyword Search Results:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Keyword Matches: 1\n",
      "   Content: LlamaIndex Basics: An Introduction for\n",
      "Retrieval-Augmented Generation\n",
      "1. Introduction to LlamaIndex\n",
      "LlamaIndexis a framework designed to connect Large Language Models (LLMs) with\n",
      "external data sources such as PDF documents, text files, databases, and...\n",
      "\n",
      "2. Keyword Matches: 1\n",
      "   Content: Instead of providing entire documents to an LLM, LlamaIndex:\n",
      "•Splits documents into smaller units\n",
      "•Stores them in an index\n",
      "•Retrieves only the most relevant parts for each query\n",
      "4. Main Components of LlamaIndex\n",
      "4.1 Document Loader\n",
      "The Document Loader...\n",
      "\n",
      "3. Keyword Matches: 1\n",
      "   Content: •Keyword Index\n",
      "•List Index\n",
      "The vector index is the most commonly used in RAG systems.\n",
      "4.4 Embeddings\n",
      "Text content is converted into dense numerical vectors known asembeddings.\n",
      "Embeddings enable:\n",
      "•Semantic similarity comparison\n",
      "•Meaning-based retrieva...\n",
      "\n",
      "\n",
      "Hybrid approach combines semantic similarity with keyword matching!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# Vector retriever (semantic search)\n",
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=5)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HYBRID SEARCH COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Vector search results\n",
    "print(\"\\nVector Search Results:\")\n",
    "print(\"-\" * 70)\n",
    "vector_nodes = vector_retriever.retrieve(question)\n",
    "for i, node in enumerate(vector_nodes, 1):\n",
    "    print(f\"\\n{i}. Score: {node.score:.4f}\")\n",
    "    print(f\"   Content: {node.get_content()[:250]}...\")\n",
    "\n",
    "# Simple keyword-based filtering\n",
    "print(\"\\n\\nKeyword Search Results:\")\n",
    "print(\"-\" * 70)\n",
    "keywords = question.lower().split()\n",
    "keyword_matches = []\n",
    "for doc in documents:\n",
    "    content = doc.get_content().lower()\n",
    "    match_count = sum(1 for kw in keywords if kw in content)\n",
    "    if match_count > 0:\n",
    "        keyword_matches.append((doc, match_count))\n",
    "\n",
    "keyword_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (doc, matches) in enumerate(keyword_matches[:3], 1):\n",
    "    print(f\"\\n{i}. Keyword Matches: {matches}\")\n",
    "    print(f\"   Content: {doc.get_content()[:250]}...\")\n",
    "\n",
    "print(\"\\n\\nHybrid approach combines semantic similarity with keyword matching!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
